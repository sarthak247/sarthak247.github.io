<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on Sarthak Thakur</title>
    <link>http://localhost:1313/blogs/</link>
    <description>Recent content in Blogs on Sarthak Thakur</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 13 Jul 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/blogs/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Building Efficient ML APIs with FastAPI: A Comprehensive Guide - Part 2</title>
      <link>http://localhost:1313/blogs/fastapi2/</link>
      <pubDate>Sat, 13 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/fastapi2/</guid>
      <description>&lt;p&gt;In the &lt;a href=&#34;../fastapi&#34;&gt;previous blog&lt;/a&gt;, I mentioned how I was able to make simple endpoints with FastAPI and how I was even able to use it to deploy a pretrained ML model. Moving forward, I will be discussing about how to make our FastAPI application more robust and efficient with some commonly used practices I found around the internet. Note that these are completely optional and for just deploying a model, even the previous blog will suffice. It&amp;rsquo;s just that I thought while I am at it, might just learn a bit more the best practices regarding FastAPI.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building Efficient ML APIs with FastAPI: A Comprehensive Guide - Part 1</title>
      <link>http://localhost:1313/blogs/fastapi/</link>
      <pubDate>Fri, 12 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/fastapi/</guid>
      <description>&lt;h2 id=&#34;introduction-to-fastapi&#34;&gt;Introduction to FastAPI&lt;/h2&gt;&#xA;&lt;p&gt;Today, I delved into FastAPI, a cutting-edge web framework designed for building APIs with Python. It piqued my interest because of its reputation for speed, ease of use, and robust integration with Python&amp;rsquo;s type system and also cause I have been looking for some time for a way to deploy my ML applications easily and with minimalistic code as I&amp;rsquo;m not a web developer and struggle with that part. Upon looking around on the internet, I discovered that people have been using something called &lt;code&gt;FastAPI&lt;/code&gt; along with &lt;code&gt;Streamlit&lt;/code&gt; for faster model deployment for demonstration purposes and so I decided to have a look at it. Hereâ€™s what I discovered about FastAPI and why it&amp;rsquo;s particularly compelling for developing APIs, especially for machine learning tasks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Attention Is All You Need - From where it all started</title>
      <link>http://localhost:1313/blogs/attention-is-all-you-need/</link>
      <pubDate>Wed, 03 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/attention-is-all-you-need/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;In the current landscape of Generative Artificial Intelligence (GenAI), new architectures and methodologies emerge with remarkable frequency. According to recent statistics from &lt;code&gt;arXiv&lt;/code&gt;, the volume of papers tagged under &lt;code&gt;LLM&lt;/code&gt; averages nine submissions daily. To comprehend these advancements fully, it is essential to delve into the seminal work that laid the groundwork for many contemporary developments in Natural Language Processing (NLP): the paper entitled &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;&amp;ldquo;Attention Is All You Need&amp;rdquo;&lt;/a&gt;. This pivotal research introduced Transformer models to the NLP domain, which have since become fundamental frameworks underpinning a myriad of advanced language models such as GPT, Llama, Claude, among others. Furthermore, the paper introduced innovative concepts like &lt;code&gt;Multi-Head attention&lt;/code&gt; and &lt;code&gt;Positional Encoding&lt;/code&gt;, which serve as cornerstone elements in the construction of modern AI architectures.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Unraveling the Power of Word Vectorization: From Text to Numbers</title>
      <link>http://localhost:1313/blogs/word-embeddings/</link>
      <pubDate>Tue, 05 Sep 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/word-embeddings/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;In the realm of Natural Language Processing (NLP), one of the key challenges is to bridge the gap between the textual world of words and the numerical world of computers. We know that computers only understand in form of numbers (0s and 1s to be precise). So how do we make sense of human language words like maybe &amp;ldquo;cat&amp;rdquo; and &amp;ldquo;car&amp;rdquo; using algorithms and data structures? This is where &lt;code&gt;word vectorization&lt;/code&gt; comes into play.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Embracing Freedom: My Journey to Creating a Personal Blog with Hugo and GitHub Pages</title>
      <link>http://localhost:1313/blogs/hello-world/</link>
      <pubDate>Mon, 04 Sep 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/hello-world/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;In a world dominated by social media platforms and curated content, the idea of having a personal space on the internet has always fascinated me. A place where I can express my thoughts, share my experiences, and have complete control over what goes in and what stays out. That&amp;rsquo;s why I decided to embark on a journey to create my own personal blog. This blog is my &amp;ldquo;Hello World&amp;rdquo; moment in the blogosphere, a simple yet profound beginning.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
