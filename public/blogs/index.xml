<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on Sarthak Thakur</title>
    <link>http://localhost:1313/blogs/</link>
    <description>Recent content in Blogs on Sarthak Thakur</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Jul 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/blogs/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Attention Is All You Need - From where it all started</title>
      <link>http://localhost:1313/blogs/attention-is-all-you-need/</link>
      <pubDate>Wed, 03 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/attention-is-all-you-need/</guid>
      <description>Introduction In the current landscape of Generative Artificial Intelligence (GenAI), new architectures and methodologies emerge with remarkable frequency. According to recent statistics from arXiv, the volume of papers tagged under LLM averages nine submissions daily. To comprehend these advancements fully, it is essential to delve into the seminal work that laid the groundwork for many contemporary developments in Natural Language Processing (NLP): the paper entitled &amp;ldquo;Attention Is All You Need&amp;rdquo;. This pivotal research introduced Transformer models to the NLP domain, which have since become fundamental frameworks underpinning a myriad of advanced language models such as GPT, Llama, Claude, among others.</description>
    </item>
    <item>
      <title>Unraveling the Power of Word Vectorization: From Text to Numbers</title>
      <link>http://localhost:1313/blogs/word-embeddings/</link>
      <pubDate>Tue, 05 Sep 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/word-embeddings/</guid>
      <description>Introduction In the realm of Natural Language Processing (NLP), one of the key challenges is to bridge the gap between the textual world of words and the numerical world of computers. We know that computers only understand in form of numbers (0s and 1s to be precise). So how do we make sense of human language words like maybe &amp;ldquo;cat&amp;rdquo; and &amp;ldquo;car&amp;rdquo; using algorithms and data structures? This is where word vectorization comes into play.</description>
    </item>
    <item>
      <title>Embracing Freedom: My Journey to Creating a Personal Blog with Hugo and GitHub Pages</title>
      <link>http://localhost:1313/blogs/hello-world/</link>
      <pubDate>Mon, 04 Sep 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/hello-world/</guid>
      <description>Introduction In a world dominated by social media platforms and curated content, the idea of having a personal space on the internet has always fascinated me. A place where I can express my thoughts, share my experiences, and have complete control over what goes in and what stays out. That&amp;rsquo;s why I decided to embark on a journey to create my own personal blog. This blog is my &amp;ldquo;Hello World&amp;rdquo; moment in the blogosphere, a simple yet profound beginning.</description>
    </item>
  </channel>
</rss>
