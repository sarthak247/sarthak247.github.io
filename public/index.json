[{"content":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn the current landscape of Generative Artificial Intelligence (GenAI), new architectures and methodologies emerge with remarkable frequency. According to recent statistics from \u003ccode\u003earXiv\u003c/code\u003e, the volume of papers tagged under \u003ccode\u003eLLM\u003c/code\u003e averages nine submissions daily. To comprehend these advancements fully, it is essential to delve into the seminal work that laid the groundwork for many contemporary developments in Natural Language Processing (NLP): the paper entitled \u003ca href=\"https://arxiv.org/abs/1706.03762\"\u003e\u0026ldquo;Attention Is All You Need\u0026rdquo;\u003c/a\u003e. This pivotal research introduced Transformer models to the NLP domain, which have since become fundamental frameworks underpinning a myriad of advanced language models such as GPT, Llama, Claude, among others. Furthermore, the paper introduced innovative concepts like \u003ccode\u003eMulti-Head attention\u003c/code\u003e and \u003ccode\u003ePositional Encoding\u003c/code\u003e, which serve as cornerstone elements in the construction of modern AI architectures.\u003c/p\u003e\n\u003cp\u003eAccording to the structure proposed for this blog, the initial focus will be on evaluating the limitations inherent in current methodologies of that time, such as \u003ccode\u003eRecurrent Neural Networks (RNNs)\u003c/code\u003e. Subsequently, attention will shift towards introducing the \u003ccode\u003eTransformer\u003c/code\u003e architecture. This discussion will meticulously dissect both the \u003ccode\u003eEncoder\u003c/code\u003e and \u003ccode\u003eDecoder\u003c/code\u003e components of the Transformer architecture, elucidating the specific functionalities of each constituent part that collectively contribute to its robust framework.\u003c/p\u003e\n\u003ch2 id=\"rnns-and-their-problems\"\u003eRNNs and their problems\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"RNN.png\" alt=\"RNN working\"\u003e\u003c/p\u003e\n\u003cp\u003eTo commence, we will first explore the operational principles of Recurrent Neural Networks (RNNs). While this discussion does not aim to provide an exhaustive tutorial on RNNs, a basic overview with a simplified diagram will be presented to illustrate their fundamental workings and inherent challenges.\u003c/p\u003e\n\u003cp\u003eRNNs are designed to sequentially map input \u003cem\u003e\u003cstrong\u003ex\u003c/strong\u003e\u003c/em\u003e to output \u003cem\u003e\u003cstrong\u003ey\u003c/strong\u003e\u003c/em\u003e, either in a recurrent or sequence-to-sequence manner. Their operation can be summarized as follows:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThey are employed to establish a mapping from \u003cem\u003e\u003cstrong\u003ex\u003c/strong\u003e\u003c/em\u003e to \u003cem\u003e\u003cstrong\u003ey\u003c/strong\u003e\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eFor a sequence comprising \u003cstrong\u003en\u003c/strong\u003e tokens, the computation unfolds across \u003cstrong\u003en\u003c/strong\u003e sequential steps.\u003c/li\u003e\n\u003cli\u003eThe process initiates with an initial or zeroth stage, where the first input token, denoted as \\(x_1\\), is fed into the initial RNN cell alongside the preceding hidden state (typically denoted as state zero in our case for first token), resulting in the production of the first output token \\(y_1\\).\u003c/li\u003e\n\u003cli\u003eThis iterative process continues: \\(h_1\\) and \\(x_2\\) become inputs to the second cell, yielding \\(y_2\\) and \\(h_2\\), and so forth, until all tokens have been processed.\u003c/li\u003e\n\u003cli\u003eConsequently, each RNN cell operates with two inputs: \\(x_i\\) (current token) and \\(h_{i-1}\\) (hidden state from the previous computation).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"problems\"\u003eProblems\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eSlow computation for long sequences\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eSince it is a sequential model, and each word depends on the words before it, the computation can become really slow in case of longer text sequences.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVanishing and Exploding Gradients\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eThe longer the chain becomes, the more will be the chances of vanishing/exploding gradients (cause of chain rule) which is not desirable as it slows down in training and also in some cases might not be able to be represented by the system (consider int32, float32, etc)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDifficulty in accessing information from long ago\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eThe first few tokens will have very less or almost negligible impact on the last tokens (loss of context). This is undersirable as can be seen in the case of RNNs that their attention span or context window is much smaller and they suffer with longer text sequences.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"transformer-for-the-rescue\"\u003eTransformer for the rescue\u003c/h2\u003e\n\u003cp\u003eAddressing the challenges highlighted by RNNs, particularly their limitations with longer sequences, Vishwani (\u003ca href=\"https://arxiv.org/search/cs?searchtype=author\u0026amp;query=Vaswani,+A\"\u003ehttps://arxiv.org/search/cs?searchtype=author\u0026amp;query=Vaswani,+A\u003c/a\u003e) and his team introduced the Transformer architecture in 2017, which effectively mitigated these issues. Unlike RNNs, Transformers streamline computation by executing a single forward pass for processing an input sentence, eliminating the need for multiple sequential passes. Furthermore, Transformers leverage a broader context window and circumvent context loss through sophisticated mechanisms such as positional encoding and attention.\u003c/p\u003e\n\u003cp\u003eThe Transformer architecture comprises two pivotal components: the \u003ccode\u003eencoder\u003c/code\u003e and the \u003ccode\u003edecoder\u003c/code\u003e. This blog will commence by exploring the functionalities of the encoder, followed by an in-depth examination of the decoder. Ultimately, we will synthesize these components to elucidate how they synergistically contribute to the comprehensive Transformer model.\u003c/p\u003e\n\u003ch2 id=\"encoder\"\u003eEncoder\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"IE.png\" alt=\"Input Embeddings\"\u003e\u003c/p\u003e\n\u003cp\u003eTo understand how an encoder works, let us first consider an input sentence, \u003ccode\u003eYour cat is a lovely cat\u003c/code\u003e.\u003c/p\u003e\n\u003ch3 id=\"step-1-input-embeddings\"\u003eStep 1: Input Embeddings\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eThe initial step involves generating \u003ccode\u003eInput Embeddings\u003c/code\u003e for the given sentence.\u003c/li\u003e\n\u003cli\u003eThis process begins with \u003ccode\u003etokenization\u003c/code\u003e, where the sentence is segmented into individual words.\u003c/li\u003e\n\u003cli\u003eSubsequently, each token is assigned a unique numerical identifier corresponding to its position within the vocabulary, as depicted in the accompanying figure.\n\u003cul\u003e\n\u003cli\u003eNotably, the input ID for \u0026ldquo;cat\u0026rdquo; remains consistent across its occurrences in the sentence.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eOriginal Sentence Token\u003c/th\u003e\n\u003cth\u003eInput ID (vocabulary position)\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eYour\u003c/td\u003e\n\u003ctd\u003e105\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ecat\u003c/td\u003e\n\u003ctd\u003e6587\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eis\u003c/td\u003e\n\u003ctd\u003e5475\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ea\u003c/td\u003e\n\u003ctd\u003e3578\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003elovely\u003c/td\u003e\n\u003ctd\u003e65\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ecat\u003c/td\u003e\n\u003ctd\u003e6578\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eThese input IDs are then transformed into embeddings of a fixed size, typically 512 in dimension.\n\u003cul\u003e\n\u003cli\u003eImportantly, these \u003ccode\u003eembeddings\u003c/code\u003e serve as trainable parameters and are subject to adjustment during training to optimize the model\u0026rsquo;s loss function.\u003c/li\u003e\n\u003cli\u003eFurthermore, we define \\(d_{model} = 512\\), representing the dimensionality of each word\u0026rsquo;s embedding vector.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"step-2-positional-embeddings\"\u003eStep 2. Positional Embeddings\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eOnce we obtain the \u003ccode\u003einput embeddings\u003c/code\u003e, our objective is for each word to encapsulate its \u003ccode\u003espatial encoding\u003c/code\u003e within the sentenceâ€”essentially ensuring that each word is aware of its positional information.\u003c/li\u003e\n\u003cli\u003eWe aim for the model to perceive words appearing close together as \u003ccode\u003eproximate\u003c/code\u003e and those appearing farther apart as \u003ccode\u003edistant\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eThe \u003ccode\u003epositional encoding\u003c/code\u003e should therefore embody a pattern that the model can assimilate. \u003cstrong\u003e(But how?)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg src=\"PE.png\" alt=\"Positional Embeddings\"\u003e\u003c/p\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eReferring to the figure above, we observe that PE (positional encoding) augments the input embedding of each word, thus forming our encoder input.\n\u003cul\u003e\n\u003cli\u003eHere, PE is a fixed-size vector of 512 dimensions, computed once and utilized consistently across all sentences during both training and inference.\u003c/li\u003e\n\u003cli\u003eThis identical PE can be applied across different sentences, serving purely as a positional marker and hence is neither learned nor unique to individual sentences.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg src=\"PE_calculation.png\" alt=\"How is Positional Embeddings calculated?\"\u003e\u003c/p\u003e\n\u003col start=\"5\"\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eBut how is positional encoding computed?\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIn their paper, the authors employ a formula involving \\(sin - cos\\) functions to generate alternating values across dimensions, resulting in vectors of length \\(d_{model}\\).\u003c/li\u003e\n\u003cli\u003eIn the paper, the authors use a \\(sin - cos\\) formula for alternating words and then create vectors of length \\(d_{model}\\).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBut why restrict to trigonometric functions? Why not consider alternatives?\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eTrigonometric functions like sine and cosine inherently depict a pattern that the model can recognize as \u003ccode\u003econtinuous\u003c/code\u003e, facilitating the model\u0026rsquo;s ability to discern relative positions. The regularity of these functions in their plots suggests a predictable pattern, which leads us to hypothesize that the model will perceive and leverage these patterns effectively.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"trigno.png\" alt=\"Trignometric Functions\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"step-3-self-attention-with-a-single-head\"\u003eStep 3. Self attention (with a single head)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eSelf-attention has been a concept predating the seminal attention paper. The authors introduced the concept of \u003ccode\u003emulti-head attention\u003c/code\u003e, which we will delve into later. However, to grasp that, it is essential first to comprehend how \u003ccode\u003esingle-head attention\u003c/code\u003e operates.\u003c/li\u003e\n\u003cli\u003eSelf-attention enables the model to \u003cstrong\u003eestablish relationships between words.\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eInput Embeddings: Capture the semantic meaning of each word.\u003c/li\u003e\n\u003cli\u003ePositional Encoding: Provide positional information within the sentence.\u003c/li\u003e\n\u003cli\u003eSelf Attention: \u003cstrong\u003eFacilitate relationships between these words.\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eTherefore, it is through self-attention that words are able to \u003ccode\u003eeffectively capture contextual nuances and their interdependencies with other words.\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\\[\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_{model}}}\\right) V\\]\u003c/p\u003e\n\u003cp\u003eIn the provided formula, Q, K, and V represent the input sentence embeddings (after positional encoding), each of size ((6, 512)\\), where \\(\\text{seq_len} = 6\\) and \\(d_{model} = 512\\).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"self_attention.png\" alt=\"Self-Attention Head\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eEach row in this resultant attention matrix not only encapsulates the meaning (via embeddings) and positional information (via PE) but also signifies the interactions and relationships between individual words.\u003c/strong\u003e\u003c/p\u003e\n\u003ch4 id=\"some-properties-of-self-attention\"\u003eSome properties of Self-attention:\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003eSelf-attention exhibits \u003ccode\u003epermutation invariance\u003c/code\u003e:\n\u003cul\u003e\n\u003cli\u003eThis property ensures that rearranging the order of words does not alter the computed values. This characteristic is highly desirable.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- raw HTML omitted --\u003e\n\u003cul\u003e\n\u003cli\u003eIn this illustration, swapping the positions of \\(B\\) and \\(C\\) does not change the values computed for \\(B\u0026rsquo;\\) and \\(C\u0026rsquo;\\); it merely repositions them in the matrix.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- raw HTML omitted --\u003e\n\u003c/li\u003e\n\u003cli\u003eAs of now, self-attention requires no additional parameters (though this changes with multi-head attention).\u003c/li\u003e\n\u003cli\u003eValues along the diagonal are expected to be the highest, reflecting each word\u0026rsquo;s strongest relationship with itself.\u003c/li\u003e\n\u003cli\u003eTo prevent certain positions from interacting, their values can be set to \\(-\\infty\\) before applying the softmax operation. This ensures that the model does not learn interactions involving those positions (since \\(e^{-\\infty} = 0\\)). This property is particularly useful in decoder settings.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"step-4-multi-head-attention\"\u003eStep 4: Multi-Head Attention\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"multi_head.png\" alt=\"Multi-Head Attention\"\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eInitially, three copies of the input embeddings are created: \\(Q, K, V\\), akin to single-head attention.\u003c/li\u003e\n\u003cli\u003eThese \\(Q, K, , V\\) undergo multiplication by parameterized matrices \\(W^q, W^k, W^v\\) to yield \\(Q\u0026rsquo;, K\u0026rsquo;, V\u0026rsquo;\\).\u003c/li\u003e\n\u003cli\u003e\\(Q\u0026rsquo;, K\u0026rsquo;, V\u0026rsquo;\\) are subsequently divided into multiple heads, following which attention is independently computed for each head using the standard procedure:\n\\[\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_{k}}}\\right) V\\]\n\\[\\text{and } head_i = Attention(QW_i^q, KW_i^k, VW_i^v)\\]\u003c/li\u003e\n\u003cli\u003eThese individual heads are then concatenated to form the \\(H\\) matrix, which is subsequently multiplied by \\(W^o\\) to produce the final output:\n\\[\\text{Multihead}(Q, K, V) = \\text{concat}(head_1, head_2, \u0026hellip;. head_h)W^o\\]\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eBut why split into multiple heads?\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eEach \\(head_i\\) processes the entire sentence but focuses on a \u003ccode\u003edistinct aspect\u003c/code\u003e of the word embeddings.\u003c/li\u003e\n\u003cli\u003eThis approach allows each head to specialize in different semantic aspects of the same word. For example, in Chinese, a word can function as a noun, verb, or adverb depending on context. By employing multiple heads, the model can capture these diverse possibilities.\u003c/li\u003e\n\u003cli\u003eThis structured approach of multi-head attention enables the model to effectively capture and leverage various semantic nuances and syntactic structures within the input data.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWhy Q, K and V and not something else?\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eThis choice follows the convention used in Python dictionaries for keys (K), values (V), and queries (Q).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"step-5-add-and-norm\"\u003eStep 5: Add and Norm\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBefore delving into the add and norm layer, it is essential to understand layer normalization.\u003c/li\u003e\n\u003cli\u003eLet\u0026rsquo;s consider a batch containing three items:\n\u003cimg src=\"norm.png\" alt=\"Add and Normalization\"\u003e\u003c/li\u003e\n\u003cli\u003eWFor each item in the batch, we compute the mean \\(\\mu_i\\) and variance \\(\\sigma^2_i\\) separately. Each value \\(x_i\\) in the item is then transformed using the following formula:\n\\[x_i = \\gamma \\frac{x - \\mu_i}{\\sqrt{\\sigma^2_i + \\epsilon}} + \\beta\\]\u003c/li\u003e\n\u003cli\u003eIntroducing two additional parameters, \\(\\gamma\\) and \\(\\beta\\), serves to introduce variability or adjustments into the data. This step is crucial because strictly confining all values between 0 and 1 may overly constrain the model.\n\u003cul\u003e\n\u003cli\u003eThe network learns to adjust these parameters \\(\\gamma\\) and \\(\\beta\\) as needed to accommodate variations in the data where appropriate.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis approach of layer normalization ensures that the model can effectively handle varying scales and distributions within the data, promoting stable and efficient training processes.\u003c/p\u003e\n\u003ch2 id=\"decoder\"\u003eDecoder\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eIn the case of the decoder, similar to the encoder, we start with \u003ccode\u003eoutput embeddings\u003c/code\u003e analogous to input embeddings in the encoder, and \u003ccode\u003epositional embeddings\u003c/code\u003e similar to PE in the encoder.\u003c/li\u003e\n\u003cli\u003eNext, instead of the regular multi-head attention in the encoder, we employ \u003ccode\u003eMASKED multi-head attention\u003c/code\u003e and \u003ccode\u003ecross-attention\u003c/code\u003e followed by an add and norm layer.\u003c/li\u003e\n\u003cli\u003eThe \\(K, V\\) inputs are derived from the encoder output, whereas the \\(Q\\) input originates from the decoder. This setup transforms the self-attention block into a \u003ccode\u003ecross-attention block\u003c/code\u003e, where the decoder\u0026rsquo;s focus extends beyond its own input to encompass information from the encoder.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"step-1-masked-multi-head-attention\"\u003eStep 1: Masked Multi-Head Attention\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eThe primary objective is to ensure the model\u0026rsquo;s \u003ccode\u003ecausality\u003c/code\u003e, meaning that the output at any given position can only depend on preceding words. (\u003cstrong\u003eHow?\u003c/strong\u003e)\u003c/li\u003e\n\u003cli\u003eTo enforce this restriction of not allowing the model to see future words, we replace all future positions with \\(-\\infty\\), as previously discussed.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eYour\u003c/th\u003e\n\u003cth\u003eCat\u003c/th\u003e\n\u003cth\u003eIs\u003c/th\u003e\n\u003cth\u003eA\u003c/th\u003e\n\u003cth\u003eLovely\u003c/th\u003e\n\u003cth\u003eCat\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eYour\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\\(-\\infty\\)\u003c/td\u003e\n\u003ctd\u003e\\(-\\infty\\)\u003c/td\u003e\n\u003ctd\u003e\\(-\\infty\\)\u003c/td\u003e\n\u003ctd\u003e\\(-\\infty\\)\u003c/td\u003e\n\u003ctd\u003e\\(-\\infty\\)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCat\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\\(-\\infty\\)\u003c/td\u003e\n\u003ctd\u003e\\(-\\infty\\)\u003c/td\u003e\n\u003ctd\u003e\\(-\\infty\\)\u003c/td\u003e\n\u003ctd\u003e\\(-\\infty\\)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eIs\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\\(-\\infty\\)\u003c/td\u003e\n\u003ctd\u003e\\(-\\infty\\)\u003c/td\u003e\n\u003ctd\u003e\\(-\\infty\\)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eA\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\\(-\\infty\\)\u003c/td\u003e\n\u003ctd\u003e\\(-\\infty\\)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eLovely\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\\(-\\infty\\)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCat\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eWhen we apply softmax on this masked attention matrix, the relations between words denoted by \\(-\\infty\\) effectively become zero. This masking mechanism ensures that the model does not attend to future words.\u003c/p\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eThe remaining components resemble those in the encoder, including a \u003ccode\u003ecross-attention block\u003c/code\u003e â€” which uses two inputs from the encoder output and one from the decoder outputâ€”and an add and norm layer. This block functions similarly to a self-attention block in the encoder, fostering interaction between encoder and decoder outputs to enhance sequence generation.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"training-the-model\"\u003eTraining the model\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eWe will consider a translation task, akin to the approach outlined in the referenced paper.\u003c/li\u003e\n\u003cli\u003eFor instance, translating \u003ccode\u003eI love you very much (en)\u003c/code\u003e to \u003ccode\u003eTi Amo Molto (it)\u003c/code\u003e.\n\u003cimg src=\"training.png\" alt=\"Training the model\"\u003e\u003c/li\u003e\n\u003cli\u003eSpecial tokens, notably \\([SOS]\\) (Start of Sentence) and \\([EOS]\\) (End of Sentence), are incorporated into our sentences.\u003c/li\u003e\n\u003cli\u003eSubsequently, we compute the \u003ccode\u003ei\u003c/code\u003enput embeddings\u003ccode\u003eand\u003c/code\u003epositional encodings` for both the encoder and the decoder.\u003c/li\u003e\n\u003cli\u003eThe encoder provides key \\((K)\\) and value \\((V)\\) vectors, which are utilized in conjunction with the masked query \\((Q)\\) from the decoder to generate decoder outputs.\u003c/li\u003e\n\u003cli\u003eThese outputs undergo transformation through a \u003ccode\u003elinear layer\u003c/code\u003e to map them into our vocabulary space.\u003c/li\u003e\n\u003cli\u003eThe resultant logits are subjected to a softmax operation to derive word predictions.\u003c/li\u003e\n\u003cli\u003eThe model\u0026rsquo;s predictions are evaluated against the actual labels using the \u003ccode\u003ecross entropy\u003c/code\u003e loss function to adjust the model weights.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eAll these operations are performed within a single time step, exemplifying the efficiency achieved in processing sequences in one pass.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"inferencing-the-model\"\u003eInferencing the model\u003c/h2\u003e\n\u003cp\u003eFor inferring from the model, we follow a sequence of steps over four distinct time steps, as detailed below:\u003c/p\u003e\n\u003ch3 id=\"time-step-1\"\u003eTime Step 1\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"infer_1.png\" alt=\"Inference Time Step - 1\"\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eInitiate with the \\([SOS]\\) token and compute logits using the decoder model.\n2.Apply softmax to these logits to predict the next word, which in this case is \u003ccode\u003eTi\u003c/code\u003e.\n\\[[SOS] \\to Ti\\]\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"time-step-2\"\u003eTime Step 2\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"infer_2.png\" alt=\"Inference Time Step - 2\"\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe encoder computation remains unchanged as the English sentence remains the same.\u003c/li\u003e\n\u003cli\u003eProceed from the current state where \\([SOS]\\ Ti\\) was generated to predict \u003ccode\u003eAmo\u003c/code\u003e.\n\\[[SOS]\\ Ti \\to Amo\\]\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"time-step-3\"\u003eTime Step 3\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"infer_3.png\" alt=\"Inference Time Step - 3\"\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eContinue the sequence from \\([SOS]\\ Ti\\ Amo\\) to predict \u003ccode\u003eMolto\u003c/code\u003e.\n\\[[SOS]\\ Ti\\ Amo \\to Molto\\]\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"time-step-4\"\u003eTime Step 4\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"infer_4.png\" alt=\"Inference Time Step - 4\"\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eConclude the sequence with \\([SOS]\\ Ti\\ Amo\\ Molto\\) resulting in the \u003ccode\u003e[EOS]\u003c/code\u003e token.\n\\[[SOS]\\ Ti\\ Amo\\ Molto \\to [EOS]\\]\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eThus, the process involves generating tokens sequentially until the \u003ccode\u003e[EOS]\u003c/code\u003e token is generated, completing the inference process.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"inference-strategy\"\u003eInference Strategy\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eAt each stage, our approach involved selecting the word with the highest probability or softmax value. This method is commonly known as \u003ccode\u003egreedy decoding\u003c/code\u003e, although it is often associated with suboptimal performance.\u003c/li\u003e\n\u003cli\u003eAlternatively, a more effective strategy entails considering the top K words and evaluating all potential subsequent words for each of them at every stage. This method, referred to as \u003ccode\u003eBeam Search\u003c/code\u003e, typically yields superior results by maintaining the top K most probable sequences throughout the process.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"references\"\u003eReferences:\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1706.03762\"\u003eAttention Is All You Need\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/watch?v=ISNdQcPhsts\"\u003eCoding a Transformer from scratch on PyTorch, with full explanation, training and inference.\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/watch?v=bCz4OMemCcA\"\u003eAttention is all you need (Transformer) - Model explanation (including math), Inference and Training\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hkproj/transformer-from-scratch-notes/blob/main/Diagrams_V2.pdf\"\u003eTransformers from scratch notes by Umar Jamil\u003c/a\u003e used for figures in this blog.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"appendix\"\u003eAppendix\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#Fixit\"\u003eAnnotated Attention Paper\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/sarthak247/Attention-Is-All-You-Need\"\u003eGitHub code repo\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"attention_notes.pdf\"\u003eHandwritten Notes\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePhoto by \u003ca href=\"https://arxiv.org/abs/1706.03762\"\u003eAttention Is All You Need\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n","description":"Explanation of Attention Is All You Need paper along with code","image":"/projects/attention.png","permalink":"http://localhost:1313/blogs/attention-is-all-you-need/","title":"Attention Is All You Need - From where it all started"},{"content":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn the realm of Natural Language Processing (NLP), one of the key challenges is to bridge the gap between the textual world of words and the numerical world of computers. We know that computers only understand in form of numbers (0s and 1s to be precise). So how do we make sense of human language words like maybe \u0026ldquo;cat\u0026rdquo; and \u0026ldquo;car\u0026rdquo; using algorithms and data structures? This is where \u003ccode\u003eword vectorization\u003c/code\u003e comes into play.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-word-vectorization\"\u003eWhy Do We Need Word Vectorization?\u003c/h2\u003e\n\u003cp\u003eOne of my favourite books is \u0026ldquo;Rich Dad, Poor Dad\u0026rdquo; by Robert Kiyosaki. Now, whenever I pick up this book, I end up taking away something from it and learning something from it. But what if I wanted to process this book using a computer? For a computer, it\u0026rsquo;s just a bunch of characters and symbols â€“ meaningless patterns. Yet, this book contains knowledge, stories, and insights as we know already. It\u0026rsquo;s written in a language, and those words convey meaning and context.\u003c/p\u003e\n\u003cp\u003eSo, how do we make computers understand this intricate web of human communication? The answer lies in the art and science of word vectorization. In essence, word vectorization is the process of converting words into numerical vectors, allowing machines to grasp the semantics, relationships, and nuances hidden within text.\u003c/p\u003e\n\u003cp\u003eSo for today, I started out to unravel the various methods used for word vectorization, starting with the classics like One-Hot Encoding, Count Vectorization (Bag of Words), and TF-IDF (Term Frequency-Inverse Document Frequency). I then ventured into the realm of Word2Vec, and was about to go through Glove as well but it was starting to become too much overwhelming for a day.\u003c/p\u003e\n\u003ch2 id=\"one-hot-encoding\"\u003eOne-Hot Encoding\u003c/h2\u003e\n\u003cp\u003eTo begin with, I started with the simplest method out there which was one-hot encoding. It takes each word in a given text and transforms it into a binary vector of 1s and 0s, where 1 means that a word is present in a sample and 0 means that it\u0026rsquo;s not. To understand it, I started out with a simple example.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s consider the following three sentences:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eSample\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e1.\u003c/td\u003e\n\u003ctd\u003eI love coding and coding is fun\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2.\u003c/td\u003e\n\u003ctd\u003eProgramming is fascinating\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e pandas \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e pd\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Define the sentences\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esentences \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;I love coding and coding is fun\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Programming is fascinating\u0026#34;\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Tokenize the sentences and build a vocabulary\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003evocabulary \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e set(word \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e sentence \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e sentences \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e word \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e sentence\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elower()\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esplit())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003evocabulary\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eremove(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;is\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Create a DataFrame to display one-hot vectors\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eone_hot_vectors \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e sentence \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e sentences:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    one_hot_vector \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e word \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e sentence\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elower()\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esplit() \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e word \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e vocabulary]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    one_hot_vectors\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(one_hot_vector)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Create a DataFrame\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edf \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pd\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eDataFrame(one_hot_vectors, columns\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003evocabulary, index \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e sentences)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Display the DataFrame\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(df)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe output of the above code is as follows:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003efascinating\u003c/th\u003e\n\u003cth\u003ecoding\u003c/th\u003e\n\u003cth\u003eprogramming\u003c/th\u003e\n\u003cth\u003efun\u003c/th\u003e\n\u003cth\u003elove\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eI love coding and coding is fun\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eProgramming is fascinating\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eHowever, as simple and intuitive this method may seem like there are some associated pros and cons with it which is why it is not used much in modern practices.\u003c/p\u003e\n\u003ch3 id=\"pros\"\u003ePros\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eSimplicity\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eOne-Hot Encoding is straightforward to implement and understand, making it an excellent choice for basic text representation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIndependence\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eEach word is represented independently, which can be useful in some machine learning models that assume feature independence.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"cons\"\u003eCons\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eHigh Dimensionality\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eWith a large vocabulary, eg. the English vocabulary, the vectors become high-dimensional, which can be computationally expensive and memory-intensive.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLack of semantics\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eOne-Hot Encoding doesn\u0026rsquo;t capture any semantic relationships between words; all words are equidistant from each other in the vector space. It also doesn\u0026rsquo;t keep any count or gives any relevance to how many times a word appears in a sentence as can be seen in our first example.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSparsity\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eAs we move towards larger vocabularies, most elements in the one-hot vectors are zeros, leading to a sparse representation, which can be inefficient in terms of storage and computation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThus, although One-Hot Encoding serves as a good starting point it is almost impractical in today\u0026rsquo;s machine learning requirements and thus we move forwards and explore some better methods like Count Vectorization and TF-IDF, which addressed some of these shortcomings if not all.\u003c/p\u003e\n\u003ch2 id=\"countvectorzer-bag-of-words\"\u003eCountVectorzer (Bag of Words):\u003c/h2\u003e\n\u003cp\u003eMoving forward, I read about the CountVectorizer or the Bag-of-Words model. In the realm of text representation, Count Vectorization, often referred to as the Bag of Words (BoW) model, takes a step beyond One-Hot Encoding. It not only acknowledges the presence or absence of words but also counts how many times each word appears in a document. This method creates a frequency-based numerical representation of text data, enabling machines to capture the importance of words based on their occurrence. We\u0026rsquo;ll explore this method as well on our previously defined sample texts.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"bag_of_words.png\" alt=\"Bag of Words\"\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e sklearn.feature_extraction.text \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e CountVectorizer\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Create a CountVectorizer instance\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003evectorizer \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e CountVectorizer()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Fit and transform the data\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eX \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e vectorizer\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efit_transform(sentences)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Get the feature names (vocabulary)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003efeature_names \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e vectorizer\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eget_feature_names_out()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Create a DataFrame to display count vectors\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecount_vectors \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pd\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eDataFrame(X\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etoarray(), columns\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003efeature_names, index \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e sentences)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Display the DataFrame\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(count_vectors)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe output of the above code will be as follows:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eand\u003c/th\u003e\n\u003cth\u003ecoding\u003c/th\u003e\n\u003cth\u003efascinating\u003c/th\u003e\n\u003cth\u003efun\u003c/th\u003e\n\u003cth\u003eis\u003c/th\u003e\n\u003cth\u003elove\u003c/th\u003e\n\u003cth\u003eprogramming\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eI love coding and coding is fun\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eProgramming is fascinating\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"pros-1\"\u003ePros\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eWord Frequency\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eCount Vectorization retains information about the frequency of words in a document, which can be valuable for tasks like text classification.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSimplicity\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eJust like One Hot Encoding, BoW is also relatively simpler and serves as a good starting point for many NLP tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"cons-1\"\u003eCons\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eLack of Semantics\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eSimilar to One-Hot Encoding, Count Vectorization doesn\u0026rsquo;t capture semantic relationships between words, treating them as independent entities.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNo word order\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eIt disregards the order of words in the document, which can be crucial for tasks like sentiment analysis or language modeling.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVocabulary Size\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eThe vocabulary size can be significant, leading to large feature spaces and potential computational inefficiency for bigger vocabularies.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSparsity\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eJust like OHE, even BoW suffers from sparsity issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStopwword Sensitivity\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eCommon words like \u0026ldquo;the,\u0026rdquo; \u0026ldquo;and,\u0026rdquo; and \u0026ldquo;in\u0026rdquo; can dominate the vector representation but may not carry meaningful information. A common practice is to first remove any stop words and then use BoW in order to make sure stopwords don\u0026rsquo;t dominate our word vectors.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThus, CountVectorizerorms a foundation for many text processing tasks, but it\u0026rsquo;s essential to recognize its limitations, especially when dealing with tasks that require capturing semantic meaning and context.\u003c/p\u003e\n\u003ch2 id=\"tf-idf\"\u003eTF-IDF\u003c/h2\u003e\n\u003cp\u003eWhile going through different methods for word vectorization, I came around the realization that the above two methods do not consider the importance of a word in a corpus and consider all words equally. In the realm of text representation, TF-IDF is a method that goes beyond simple word counts like these previous methods. It stands for Term Frequency-Inverse Document Frequency and is designed to capture not only the frequency of words in a document but also their importance in the context of a corpus which is where previous methods like One-Hot and CountVectorizer fail as they treated all words equally. As a result, both of those methods cannot distinguish very common words or rare words. TF-IDF gives a measure that takes the importance of a word into consideration depending on how frequently it occurs in a document or corpus.\u003c/p\u003e\n\u003cp\u003eTo understand TF-IDF, first we need to understand two terms:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTerm Frequency (TF)\u003c/strong\u003e: This component measures the frequency of a term (word) within a document. It rewards words that appear frequently within a document. It can be calculated as the ratio of the word\u0026rsquo;s occurrences in a document to the total word count in that document.\nl\u003c/li\u003e\n\u003c/ul\u003e\n$$TF(term)=\\frac{\\text{Number of times term appears in a document}}{\\text{Total number of items in the document}}$$\n\u003cp\u003eFor example, consider our previous example \u003ccode\u003eI love coding and coding is fun.\u003c/code\u003e Here, TF(coding) is 1/6 as the word \u003ccode\u003eand\u003c/code\u003e has been ignored.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eInverse Document Frequeny\u003c/strong\u003e: This component measures the rarity or importance of a word across all the documents. It is the log of the inverse of the document frequency where document frequency tells us the number of documents which contain a particular word.\u003c/li\u003e\n\u003c/ul\u003e\n$$DF(term)=\\frac{\\text{Documents containing our term}}{\\text{Total number of documents}}$$\n\u003cp\u003eThus, DF tells us about the proportion of documents which contain our word of interest. Thus, we inverse it to make sure that the more common a word is, example stopwords, the less score it gets and a logarithm is taken to dampen or reduce it\u0026rsquo;s effect on the final calculation.\u003c/p\u003e\n\u003c!-- raw HTML omitted --\u003e\n$$IDF(term)=\\log{\\bigg(\\frac{\\text{Total number of documents}}{\\text{Documents containing our term}}\\bigg)}$$\n\u003cp\u003eThus, Inverse Document Frequency (IDF) is a measure of how unique or significant a word is across a collection of documents. It can be computed as the logarithm of the total number of documents divided by the number of documents in which the word occurs, effectively quantifying the word\u0026rsquo;s rarity and importance in the entire document collection.\u003c/p\u003e\n\u003ch3 id=\"pros-2\"\u003ePros\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWord Importance\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTF-IDF captures the importance of words within specific documents and their significance in the broader corpus, making it suitable for tasks like text classification and information retrieval.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWeighting\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIt assigns weights to words based on their relevance, which can help algorithms focus on meaningful terms and disregard common stopwords.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTerm Discrimination\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIt can effectively discriminate between terms that are common across all documents and those that are unique or rare, thus emphasizing the distinctive features of documents.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"cons-2\"\u003eCons\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eLack of Semantics\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eLike previous methods, TF-IDF doesn\u0026rsquo;t capture semantic relationships between words or consider word order.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLack of Interpretibility\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eThe resulting TF-IDF values are not as intuitive or interpretable as raw word counts.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDocument Length Bias\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eLonger documents may have higher TF-IDF values simply due to their length, potentially leading to biased representations.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRare Term Issues\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eExtremely rare terms or typos may receive very high TF-IDF scores, which can lead to noise in the representation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHowever, despite its limitations TF-IDF remains a valuable and widely used text representation technique, particularly for tasks that require capturing the importance of words within documents and distinguishing between common and rare terms and also solves issues from earlier methods like BoW or One-Hot Encoding.\u003c/p\u003e\n\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eIn this blog, I delved into the world of text vectorization techniques, exploring three fundamental count-based methods: One-Hot encoding, CountVectorizer, and TF-IDF. I dissected these approaches, highlighting their inner workings and discussing the advantages and disadvantages of each.\u003c/p\u003e\n\u003cp\u003eOne-Hot encoding, with its simplicity and binary representation, is a valuable tool for certain tasks. CountVectorizer, on the other hand, provides a straightforward way to capture word frequency, while TF-IDF takes it a step further by considering the importance of words within documents and across a corpus.\u003c/p\u003e\n\u003cp\u003eIn the world of NLP, there\u0026rsquo;s no one-size-fits-all solution. Each method explored today has its strengths and weaknesses, and the art of NLP lies in selecting the right tool for the job. By understanding One-Hot encoding, CountVectorizer, and TF-IDF, one is equipped to make informed decisions that will drive the success of your text analysis projects. However, for more advanced NLP tasks that demand a deeper understanding of word semantics and context, methods like word embeddings and transformer-based models are often preferred which will be discussed later. Also, all the code used for this blog post can be found \u003ca href=\"Word_Vectors.ipynb\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"references\"\u003eReferences:\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://www.analyticsvidhya.com/blog/2021/06/part-5-step-by-step-guide-to-master-nlp-text-vectorization-approaches/\"\u003ePart 5: Step by Step Guide to Master NLP â€“ Word Embedding and Text Vectorization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://neptune.ai/blog/vectorization-techniques-in-nlp-guide\"\u003eVectorization Techniques in NLP\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePhoto by \u003ca href=\"https://projector.tensorflow.org/?_gl=1*1vlx0lq*_ga*MTI1OTYyNTE5NS4xNjYyOTc4NjE4*_ga_W0YLR4190T*MTY5MzkwMjgxMi40LjEuMTY5MzkwMzMyOS4wLjAuMA..\"\u003eEmbedding Projector\u003c/a\u003e on \u003ca href=\"https://www.tensorflow.org/text/tutorials/word2vec\"\u003eTensorFlow\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n","description":"Exploring and Unlocking Language for Machines: A Journey Through Various Word Vectorization Methods from classical methods like One-Hot Encoding to Modern Methods like Word2Vec","image":"/blogs/word-embeddings/cover.png","permalink":"http://localhost:1313/blogs/word-embeddings/","title":"Unraveling the Power of Word Vectorization: From Text to Numbers"},{"content":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn a world dominated by social media platforms and curated content, the idea of having a personal space on the internet has always fascinated me. A place where I can express my thoughts, share my experiences, and have complete control over what goes in and what stays out. That\u0026rsquo;s why I decided to embark on a journey to create my own personal blog. This blog is my \u0026ldquo;Hello World\u0026rdquo; moment in the blogosphere, a simple yet profound beginning.\u003c/p\u003e\n\u003ch2 id=\"motivation\"\u003eMotivation\u003c/h2\u003e\n\u003cp\u003eThe motivation behind creating a personal blog stemmed from a desire for creative freedom and self-expression. Here are some reasons that fueled my motivation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eFreedom of Expression:\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eUnlike social media platforms with their character limits and algorithms, a personal blog gives me the freedom to express myself without constraints. I can write long-form content, delve into topics that interest me deeply, and share my unique perspective wihtout the worry of being banned or restricted.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e\u003cstrong\u003eOwnership and Control:\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eWith a personal blog, I have complete ownership and control over my content. I don\u0026rsquo;t have to worry about my content being buried in a newsfeed or subject to changing platform policies. It\u0026rsquo;s my digital space, and I make the rules.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003e\u003cstrong\u003eLearning and Growth:\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eBuilding and maintaining a blog is a learning experience. From setting up the technical infrastructure to crafting compelling content, I saw this as an opportunity for personal growth and development.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003e\u003cstrong\u003eMaintaining My Journal:\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eAs I explore my way in this LLM world, I get to know a lot of new things on a daily basis and maintaining a blog is like my online catalog for everything I learn so that I can come back and visit it sometime later.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"how-to\"\u003eHow-To\u003c/h2\u003e\n\u003cp\u003eTo begin with, When I decided to embark on the journey of creating my own personal blog, one of the first decisions I had to make was choosing the right static site generator. After some research and contemplation, I found myself torn between two popular options: \u003ccode\u003eHugo\u003c/code\u003e and \u003ccode\u003eJekyll\u003c/code\u003e. In this blog post, I\u0026rsquo;ll share my thought process, the pros and cons I considered, and ultimately why I chose Hugo as my platform for self-expression.\u003c/p\u003e\n\u003ch3 id=\"pros-and-cons-of-hugo\"\u003ePros and Cons of Hugo\u003c/h3\u003e\n\u003ch4 id=\"pros\"\u003ePros:\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSpeed and Performance:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHugo is renowned for its blazing-fast build times. It\u0026rsquo;s built with Go, a statically typed, compiled language, which translates into remarkable speed when generating your website. This means quicker updates and reduced wait times.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFlexibility and Customization:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHugo provides a high degree of flexibility in terms of theme selection and customization. With a wide variety of themes available, one can easily find one that aligns with their vision and branding. Customizing themes and layouts is also straightforward, thanks to Hugo\u0026rsquo;s modular structure.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eLarge and Active Community:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe Hugo community is vibrant and welcoming. One will find extensive documentation, forums, and a wealth of online resources to help one troubleshoot issues and enhance their blog. The active development community ensures that Hugo stays up to date with the latest web technologies.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch4 id=\"cons\"\u003eCons:\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSteep Learning Curve:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHugo\u0026rsquo;s speed and efficiency come at the cost of complexity. For beginners, the learning curve can be steep, especially if they\u0026rsquo;re new to Markdown, Git, and the command line interface.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eLess Built-in Features:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCompared to some other static site generators, Hugo offers fewer built-in features and plugins. While this can be an advantage for simplicity, it may require additional work if you want to add complex functionality to your blog.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"the-decision-why-hugo\"\u003eThe Decision: Why Hugo?\u003c/h3\u003e\n\u003cp\u003eAfter carefully weighing the pros and cons, I decided to go with Hugo for several reasons:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSpeed Matters:\u003c/strong\u003e Hugo\u0026rsquo;s exceptional speed was a game-changer for me. I wanted the ability to publish content quickly without waiting for the site to rebuild, and Hugo\u0026rsquo;s near-instantaneous build times offered exactly that.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCommunity Support:\u003c/strong\u003e The active Hugo community was reassuring. I knew that if I encountered any issues or needed help with customization, there was a supportive community to turn to.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCustomization Potential:\u003c/strong\u003e Hugo\u0026rsquo;s flexibility and ease of theme customization aligned perfectly with my vision for a personalized blog. I wanted a blog that not only showcased my content but also reflected my unique style and personality.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"setting-up-hugo-a-step-by-step-guide\"\u003eSetting Up Hugo: A Step-by-Step Guide\u003c/h3\u003e\n\u003ch4 id=\"1-installing-go-and-hugo-extended\"\u003e1. Installing Go and Hugo Extended\u003c/h4\u003e\n\u003cp\u003eBefore diving into Hugo, I ensured I had the necessary prerequisites in place:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eInstall Go:\u003c/strong\u003e Hugo is built with Go, so you\u0026rsquo;ll need to \u003ca href=\"https://golang.org/doc/install\"\u003einstall Go\u003c/a\u003e if you haven\u0026rsquo;t already.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eInstall Hugo Extended:\u003c/strong\u003e Hugo Extended is the version of Hugo that includes additional features like SCSS processing. You can \u003ca href=\"https://github.com/gohugoio/hugo/releases\"\u003edownload it here\u003c/a\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"2-selecting-a-theme\"\u003e2. Selecting a Theme\u003c/h4\u003e\n\u003cp\u003eChoosing the right theme for your blog is crucial. Hugo offers a wide selection of themes, both in its official theme gallery and on platforms like GitHub. Spend some time exploring themes that resonate with your style and content. For my purposes I chose \u003ca href=\"https://github.com/CaiJimmy/hugo-theme-stack\"\u003eJimmy\u0026rsquo;s Stack theme\u003c/a\u003e as I liked it\u0026rsquo;s design and it is exactly what I needed from my blog.\u003c/p\u003e\n\u003ch4 id=\"3-cloning-and-customizing-the-theme\"\u003e3. Cloning and Customizing the Theme\u003c/h4\u003e\n\u003cp\u003eOnce I had found the perfect theme, I had clone its repository. Stack\u0026rsquo;s README was more than sufficient to get me started with this. After cloning and setting up, I started customizing the theme by modifying templates, colors, fonts, and adding my own branding elements along with my own social links and avatar.\u003c/p\u003e\n\u003ch4 id=\"4-pushing-to-github\"\u003e4. Pushing to GitHub\u003c/h4\u003e\n\u003cp\u003eAfter making the changes I needed, I pushed the site to GitHub. I had expected it to work out of the box just as one might expect with Jekyll but in case of Hugo it didn\u0026rsquo;t go as expected. I had to make some changes which I have described in the next step.\u003c/p\u003e\n\u003ch4 id=\"deploying-on-github-pages\"\u003eDeploying on GitHub Pages\u003c/h4\u003e\n\u003cp\u003eAfter going through Hugo\u0026rsquo;s documentation for GitHub pages, I got to know that in order to deploy my site automatically with GitHub pages I need to setup actions and provide it with a custom YAML which was given in Hugo\u0026rsquo;s documentation \u003ca href=\"https://gohugo.io/hosting-and-deployment/hosting-on-github/\"\u003ehere\u003c/a\u003e. After going through the given steps and mentioning the branch which I would be using for deployment, I was able to setup an automatic routine for deployment. Now, each time I make a push to this repo, it will be automatically built and deployed. :grin:\u003c/p\u003e\n\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eWith these steps, I was well on my way to creating a personal blog that would be a canvas for my thoughts, experiences, and creative expressions. My journey to create a personal blog with Hugo and GitHub Pages was a fulfilling experience. It allowed me to exercise my creativity, establish a digital presence, and embrace the freedom of self-expression. The decision to choose Hugo, with its speed, flexibility, and vibrant community, felt like the right one for my journey of self-expression and creativity. As I embark on this blogging adventure, I look forward to sharing my thoughts, insights, and experiences with the world while maintaining complete control over my digital space. This \u0026ldquo;Hello World\u0026rdquo; blog is just the beginning of an exciting journey in the blogosphere and a starting point for maintaining my jorunal about LLM and Generative AI research. Cheers :v:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePhoto by \u003ca href=\"https://unsplash.com/@pawel_czerwinski\"\u003ePawel Czerwinski\u003c/a\u003e on \u003ca href=\"https://unsplash.com/\"\u003eUnsplash\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n","description":"Setup Hugo + GitHub Pages based personal blog","image":"/blogs/hello-world/cover.jpg","permalink":"http://localhost:1313/blogs/hello-world/","title":"Embracing Freedom: My Journey to Creating a Personal Blog with Hugo and GitHub Pages"}]