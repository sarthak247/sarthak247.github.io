<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Transformers on Sarthak Thakur</title>
    <link>http://localhost:1313/categories/transformers/</link>
    <description>Recent content in Transformers on Sarthak Thakur</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Jul 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/transformers/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Attention Is All You Need - From where it all started</title>
      <link>http://localhost:1313/blogs/attention-is-all-you-need/</link>
      <pubDate>Wed, 03 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/attention-is-all-you-need/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;In the current landscape of Generative Artificial Intelligence (GenAI), new architectures and methodologies emerge with remarkable frequency. According to recent statistics from &lt;code&gt;arXiv&lt;/code&gt;, the volume of papers tagged under &lt;code&gt;LLM&lt;/code&gt; averages nine submissions daily. To comprehend these advancements fully, it is essential to delve into the seminal work that laid the groundwork for many contemporary developments in Natural Language Processing (NLP): the paper entitled &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;&amp;ldquo;Attention Is All You Need&amp;rdquo;&lt;/a&gt;. This pivotal research introduced Transformer models to the NLP domain, which have since become fundamental frameworks underpinning a myriad of advanced language models such as GPT, Llama, Claude, among others. Furthermore, the paper introduced innovative concepts like &lt;code&gt;Multi-Head attention&lt;/code&gt; and &lt;code&gt;Positional Encoding&lt;/code&gt;, which serve as cornerstone elements in the construction of modern AI architectures.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
