[{"content":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn the realm of Natural Language Processing (NLP), one of the key challenges is to bridge the gap between the textual world of words and the numerical world of computers. We know that computers only understand in form of numbers (0s and 1s to be precise). So how do we make sense of human language words like maybe \u0026ldquo;cat\u0026rdquo; and \u0026ldquo;car\u0026rdquo; using algorithms and data structures? This is where \u003ccode\u003eword vectorization\u003c/code\u003e comes into play.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-word-vectorization\"\u003eWhy Do We Need Word Vectorization?\u003c/h2\u003e\n\u003cp\u003eOne of my favourite books is \u0026ldquo;Rich Dad, Poor Dad\u0026rdquo; by Robert Kiyosaki. Now, whenever I pick up this book, I end up taking away something from it and learning something from it. But what if I wanted to process this book using a computer? For a computer, it\u0026rsquo;s just a bunch of characters and symbols â€“ meaningless patterns. Yet, this book contains knowledge, stories, and insights as we know already. It\u0026rsquo;s written in a language, and those words convey meaning and context.\u003c/p\u003e\n\u003cp\u003eSo, how do we make computers understand this intricate web of human communication? The answer lies in the art and science of word vectorization. In essence, word vectorization is the process of converting words into numerical vectors, allowing machines to grasp the semantics, relationships, and nuances hidden within text.\u003c/p\u003e\n\u003cp\u003eSo for today, I started out to unravel the various methods used for word vectorization, starting with the classics like One-Hot Encoding, Count Vectorization (Bag of Words), and TF-IDF (Term Frequency-Inverse Document Frequency). I then ventured into the realm of Word2Vec, and was about to go through Glove as well but it was starting to become too much overwhelming for a day.\u003c/p\u003e\n\u003ch2 id=\"one-hot-encoding\"\u003eOne-Hot Encoding\u003c/h2\u003e\n\u003cp\u003eTo begin with, I started with the simplest method out there which was one-hot encoding. It takes each word in a given text and transforms it into a binary vector of 1s and 0s, where 1 means that a word is present in a sample and 0 means that it\u0026rsquo;s not. To understand it, I started out with a simple example.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s consider the following three sentences:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eSample\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e1.\u003c/td\u003e\n\u003ctd\u003eI love coding and coding is fun\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2.\u003c/td\u003e\n\u003ctd\u003eProgramming is fascinating\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e pandas \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e pd\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Define the sentences\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esentences \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;I love coding and coding is fun\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Programming is fascinating\u0026#34;\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Tokenize the sentences and build a vocabulary\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003evocabulary \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e set(word \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e sentence \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e sentences \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e word \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e sentence\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elower()\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esplit())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003evocabulary\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eremove(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;is\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Create a DataFrame to display one-hot vectors\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eone_hot_vectors \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e sentence \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e sentences:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    one_hot_vector \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e word \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e sentence\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elower()\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esplit() \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e word \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e vocabulary]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    one_hot_vectors\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(one_hot_vector)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Create a DataFrame\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edf \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pd\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eDataFrame(one_hot_vectors, columns\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003evocabulary, index \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e sentences)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Display the DataFrame\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(df)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe output of the above code is as follows:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003efascinating\u003c/th\u003e\n\u003cth\u003ecoding\u003c/th\u003e\n\u003cth\u003eprogramming\u003c/th\u003e\n\u003cth\u003efun\u003c/th\u003e\n\u003cth\u003elove\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eI love coding and coding is fun\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eProgramming is fascinating\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eHowever, as simple and intuitive this method may seem like there are some associated pros and cons with it which is why it is not used much in modern practices.\u003c/p\u003e\n\u003ch3 id=\"pros\"\u003ePros\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eSimplicity\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eOne-Hot Encoding is straightforward to implement and understand, making it an excellent choice for basic text representation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIndependence\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eEach word is represented independently, which can be useful in some machine learning models that assume feature independence.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"cons\"\u003eCons\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eHigh Dimensionality\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eWith a large vocabulary, eg. the English vocabulary, the vectors become high-dimensional, which can be computationally expensive and memory-intensive.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLack of semantics\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eOne-Hot Encoding doesn\u0026rsquo;t capture any semantic relationships between words; all words are equidistant from each other in the vector space. It also doesn\u0026rsquo;t keep any count or gives any relevance to how many times a word appears in a sentence as can be seen in our first example.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSparsity\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eAs we move towards larger vocabularies, most elements in the one-hot vectors are zeros, leading to a sparse representation, which can be inefficient in terms of storage and computation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThus, although One-Hot Encoding serves as a good starting point it is almost impractical in today\u0026rsquo;s machine learning requirements and thus we move forwards and explore some better methods like Count Vectorization and TF-IDF, which addressed some of these shortcomings if not all.\u003c/p\u003e\n\u003ch2 id=\"countvectorzer-bag-of-words\"\u003eCountVectorzer (Bag of Words):\u003c/h2\u003e\n\u003cp\u003eMoving forward, I read about the CountVectorizer or the Bag-of-Words model. In the realm of text representation, Count Vectorization, often referred to as the Bag of Words (BoW) model, takes a step beyond One-Hot Encoding. It not only acknowledges the presence or absence of words but also counts how many times each word appears in a document. This method creates a frequency-based numerical representation of text data, enabling machines to capture the importance of words based on their occurrence. We\u0026rsquo;ll explore this method as well on our previously defined sample texts.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"bag_of_words.png\" alt=\"Bag of Words\"\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e sklearn.feature_extraction.text \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e CountVectorizer\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Create a CountVectorizer instance\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003evectorizer \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e CountVectorizer()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Fit and transform the data\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eX \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e vectorizer\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efit_transform(sentences)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Get the feature names (vocabulary)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003efeature_names \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e vectorizer\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eget_feature_names_out()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Create a DataFrame to display count vectors\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecount_vectors \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pd\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eDataFrame(X\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etoarray(), columns\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003efeature_names, index \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e sentences)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Display the DataFrame\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(count_vectors)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe output of the above code will be as follows:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eand\u003c/th\u003e\n\u003cth\u003ecoding\u003c/th\u003e\n\u003cth\u003efascinating\u003c/th\u003e\n\u003cth\u003efun\u003c/th\u003e\n\u003cth\u003eis\u003c/th\u003e\n\u003cth\u003elove\u003c/th\u003e\n\u003cth\u003eprogramming\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eI love coding and coding is fun\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eProgramming is fascinating\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"pros-1\"\u003ePros\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eWord Frequency\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eCount Vectorization retains information about the frequency of words in a document, which can be valuable for tasks like text classification.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSimplicity\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eJust like One Hot Encoding, BoW is also relatively simpler and serves as a good starting point for many NLP tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"cons-1\"\u003eCons\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eLack of Semantics\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eSimilar to One-Hot Encoding, Count Vectorization doesn\u0026rsquo;t capture semantic relationships between words, treating them as independent entities.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNo word order\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eIt disregards the order of words in the document, which can be crucial for tasks like sentiment analysis or language modeling.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVocabulary Size\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eThe vocabulary size can be significant, leading to large feature spaces and potential computational inefficiency for bigger vocabularies.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSparsity\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eJust like OHE, even BoW suffers from sparsity issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStopwword Sensitivity\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eCommon words like \u0026ldquo;the,\u0026rdquo; \u0026ldquo;and,\u0026rdquo; and \u0026ldquo;in\u0026rdquo; can dominate the vector representation but may not carry meaningful information. A common practice is to first remove any stop words and then use BoW in order to make sure stopwords don\u0026rsquo;t dominate our word vectors.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThus, CountVectorizerorms a foundation for many text processing tasks, but it\u0026rsquo;s essential to recognize its limitations, especially when dealing with tasks that require capturing semantic meaning and context.\u003c/p\u003e\n\u003ch2 id=\"tf-idf\"\u003eTF-IDF\u003c/h2\u003e\n\u003cp\u003eWhile going through different methods for word vectorization, I came around the realization that the above two methods do not consider the importance of a word in a corpus and consider all words equally. In the realm of text representation, TF-IDF is a method that goes beyond simple word counts like these previous methods. It stands for Term Frequency-Inverse Document Frequency and is designed to capture not only the frequency of words in a document but also their importance in the context of a corpus which is where previous methods like One-Hot and CountVectorizer fail as they treated all words equally. As a result, both of those methods cannot distinguish very common words or rare words. TF-IDF gives a measure that takes the importance of a word into consideration depending on how frequently it occurs in a document or corpus.\u003c/p\u003e\n\u003cp\u003eTo understand TF-IDF, first we need to understand two terms:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTerm Frequency (TF)\u003c/strong\u003e: This component measures the frequency of a term (word) within a document. It rewards words that appear frequently within a document. It can be calculated as the ratio of the word\u0026rsquo;s occurrences in a document to the total word count in that document.\nl\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$$TF(term)=\\frac{\\text{Number of times term appears in a document}}{\\text{Total number of items in the document}}$$\u003c/p\u003e\n\u003cp\u003eFor example, consider our previous example \u003ccode\u003eI love coding and coding is fun.\u003c/code\u003e Here, TF(coding) is 1/6 as the word \u003ccode\u003eand\u003c/code\u003e has been ignored.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eInverse Document Frequeny\u003c/strong\u003e: This component measures the rarity or importance of a word across all the documents. It is the log of the inverse of the document frequency where document frequency tells us the number of documents which contain a particular word.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$$DF(term)=\\frac{\\text{Documents containing our term}}{\\text{Total number of documents}}$$\u003c/p\u003e\n\u003cp\u003eThus, DF tells us about the proportion of documents which contain our word of interest. Thus, we inverse it to make sure that the more common a word is, example stopwords, the less score it gets and a logarithm is taken to dampen or reduce it\u0026rsquo;s effect on the final calculation.\u003c/p\u003e\n\u003c!-- $$IDF(term)=\\frac{\\texx{Total number of documents}}{Documents containing our term}$$ --\u003e\n\u003cp\u003e$$IDF(term)=\\log{\\bigg(\\frac{\\text{Total number of documents}}{\\text{Documents containing our term}}\\bigg)}$$\u003c/p\u003e\n\u003cp\u003eThus, Inverse Document Frequency (IDF) is a measure of how unique or significant a word is across a collection of documents. It can be computed as the logarithm of the total number of documents divided by the number of documents in which the word occurs, effectively quantifying the word\u0026rsquo;s rarity and importance in the entire document collection.\u003c/p\u003e\n\u003ch3 id=\"pros-2\"\u003ePros\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWord Importance\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTF-IDF captures the importance of words within specific documents and their significance in the broader corpus, making it suitable for tasks like text classification and information retrieval.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWeighting\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIt assigns weights to words based on their relevance, which can help algorithms focus on meaningful terms and disregard common stopwords.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTerm Discrimination\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIt can effectively discriminate between terms that are common across all documents and those that are unique or rare, thus emphasizing the distinctive features of documents.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"cons-2\"\u003eCons\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eLack of Semantics\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eLike previous methods, TF-IDF doesn\u0026rsquo;t capture semantic relationships between words or consider word order.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLack of Interpretibility\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eThe resulting TF-IDF values are not as intuitive or interpretable as raw word counts.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDocument Length Bias\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eLonger documents may have higher TF-IDF values simply due to their length, potentially leading to biased representations.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRare Term Issues\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eExtremely rare terms or typos may receive very high TF-IDF scores, which can lead to noise in the representation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHowever, despite its limitations TF-IDF remains a valuable and widely used text representation technique, particularly for tasks that require capturing the importance of words within documents and distinguishing between common and rare terms and also solves issues from earlier methods like BoW or One-Hot Encoding.\u003c/p\u003e\n\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eIn this blog, I delved into the world of text vectorization techniques, exploring three fundamental count-based methods: One-Hot encoding, CountVectorizer, and TF-IDF. I dissected these approaches, highlighting their inner workings and discussing the advantages and disadvantages of each.\u003c/p\u003e\n\u003cp\u003eOne-Hot encoding, with its simplicity and binary representation, is a valuable tool for certain tasks. CountVectorizer, on the other hand, provides a straightforward way to capture word frequency, while TF-IDF takes it a step further by considering the importance of words within documents and across a corpus.\u003c/p\u003e\n\u003cp\u003eIn the world of NLP, there\u0026rsquo;s no one-size-fits-all solution. Each method explored today has its strengths and weaknesses, and the art of NLP lies in selecting the right tool for the job. By understanding One-Hot encoding, CountVectorizer, and TF-IDF, one is equipped to make informed decisions that will drive the success of your text analysis projects. However, for more advanced NLP tasks that demand a deeper understanding of word semantics and context, methods like word embeddings and transformer-based models are often preferred which will be discussed later. Also, all the code used for this blog post can be found \u003ca href=\"Word_Vectors.ipynb\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"references\"\u003eReferences:\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://www.analyticsvidhya.com/blog/2021/06/part-5-step-by-step-guide-to-master-nlp-text-vectorization-approaches/\"\u003ePart 5: Step by Step Guide to Master NLP â€“ Word Embedding and Text Vectorization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://neptune.ai/blog/vectorization-techniques-in-nlp-guide\"\u003eVectorization Techniques in NLP\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePhoto by \u003ca href=\"https://projector.tensorflow.org/?_gl=1*1vlx0lq*_ga*MTI1OTYyNTE5NS4xNjYyOTc4NjE4*_ga_W0YLR4190T*MTY5MzkwMjgxMi40LjEuMTY5MzkwMzMyOS4wLjAuMA..\"\u003eEmbedding Projector\u003c/a\u003e on \u003ca href=\"https://www.tensorflow.org/text/tutorials/word2vec\"\u003eTensorFlow\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n","description":"Exploring and Unlocking Language for Machines: A Journey Through Various Word Vectorization Methods from classical methods like One-Hot Encoding to Modern Methods like Word2Vec","image":"/blogs/word-embeddings/cover.png","permalink":"http://localhost:1313/blogs/word-embeddings/","title":"Unraveling the Power of Word Vectorization: From Text to Numbers"},{"content":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn a world dominated by social media platforms and curated content, the idea of having a personal space on the internet has always fascinated me. A place where I can express my thoughts, share my experiences, and have complete control over what goes in and what stays out. That\u0026rsquo;s why I decided to embark on a journey to create my own personal blog. This blog is my \u0026ldquo;Hello World\u0026rdquo; moment in the blogosphere, a simple yet profound beginning.\u003c/p\u003e\n\u003ch2 id=\"motivation\"\u003eMotivation\u003c/h2\u003e\n\u003cp\u003eThe motivation behind creating a personal blog stemmed from a desire for creative freedom and self-expression. Here are some reasons that fueled my motivation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eFreedom of Expression:\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eUnlike social media platforms with their character limits and algorithms, a personal blog gives me the freedom to express myself without constraints. I can write long-form content, delve into topics that interest me deeply, and share my unique perspective wihtout the worry of being banned or restricted.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e\u003cstrong\u003eOwnership and Control:\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eWith a personal blog, I have complete ownership and control over my content. I don\u0026rsquo;t have to worry about my content being buried in a newsfeed or subject to changing platform policies. It\u0026rsquo;s my digital space, and I make the rules.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003e\u003cstrong\u003eLearning and Growth:\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eBuilding and maintaining a blog is a learning experience. From setting up the technical infrastructure to crafting compelling content, I saw this as an opportunity for personal growth and development.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003e\u003cstrong\u003eMaintaining My Journal:\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eAs I explore my way in this LLM world, I get to know a lot of new things on a daily basis and maintaining a blog is like my online catalog for everything I learn so that I can come back and visit it sometime later.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"how-to\"\u003eHow-To\u003c/h2\u003e\n\u003cp\u003eTo begin with, When I decided to embark on the journey of creating my own personal blog, one of the first decisions I had to make was choosing the right static site generator. After some research and contemplation, I found myself torn between two popular options: \u003ccode\u003eHugo\u003c/code\u003e and \u003ccode\u003eJekyll\u003c/code\u003e. In this blog post, I\u0026rsquo;ll share my thought process, the pros and cons I considered, and ultimately why I chose Hugo as my platform for self-expression.\u003c/p\u003e\n\u003ch3 id=\"pros-and-cons-of-hugo\"\u003ePros and Cons of Hugo\u003c/h3\u003e\n\u003ch4 id=\"pros\"\u003ePros:\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSpeed and Performance:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHugo is renowned for its blazing-fast build times. It\u0026rsquo;s built with Go, a statically typed, compiled language, which translates into remarkable speed when generating your website. This means quicker updates and reduced wait times.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFlexibility and Customization:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHugo provides a high degree of flexibility in terms of theme selection and customization. With a wide variety of themes available, one can easily find one that aligns with their vision and branding. Customizing themes and layouts is also straightforward, thanks to Hugo\u0026rsquo;s modular structure.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eLarge and Active Community:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe Hugo community is vibrant and welcoming. One will find extensive documentation, forums, and a wealth of online resources to help one troubleshoot issues and enhance their blog. The active development community ensures that Hugo stays up to date with the latest web technologies.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch4 id=\"cons\"\u003eCons:\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSteep Learning Curve:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHugo\u0026rsquo;s speed and efficiency come at the cost of complexity. For beginners, the learning curve can be steep, especially if they\u0026rsquo;re new to Markdown, Git, and the command line interface.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eLess Built-in Features:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCompared to some other static site generators, Hugo offers fewer built-in features and plugins. While this can be an advantage for simplicity, it may require additional work if you want to add complex functionality to your blog.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"the-decision-why-hugo\"\u003eThe Decision: Why Hugo?\u003c/h3\u003e\n\u003cp\u003eAfter carefully weighing the pros and cons, I decided to go with Hugo for several reasons:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSpeed Matters:\u003c/strong\u003e Hugo\u0026rsquo;s exceptional speed was a game-changer for me. I wanted the ability to publish content quickly without waiting for the site to rebuild, and Hugo\u0026rsquo;s near-instantaneous build times offered exactly that.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCommunity Support:\u003c/strong\u003e The active Hugo community was reassuring. I knew that if I encountered any issues or needed help with customization, there was a supportive community to turn to.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCustomization Potential:\u003c/strong\u003e Hugo\u0026rsquo;s flexibility and ease of theme customization aligned perfectly with my vision for a personalized blog. I wanted a blog that not only showcased my content but also reflected my unique style and personality.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"setting-up-hugo-a-step-by-step-guide\"\u003eSetting Up Hugo: A Step-by-Step Guide\u003c/h3\u003e\n\u003ch4 id=\"1-installing-go-and-hugo-extended\"\u003e1. Installing Go and Hugo Extended\u003c/h4\u003e\n\u003cp\u003eBefore diving into Hugo, I ensured I had the necessary prerequisites in place:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eInstall Go:\u003c/strong\u003e Hugo is built with Go, so you\u0026rsquo;ll need to \u003ca href=\"https://golang.org/doc/install\"\u003einstall Go\u003c/a\u003e if you haven\u0026rsquo;t already.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eInstall Hugo Extended:\u003c/strong\u003e Hugo Extended is the version of Hugo that includes additional features like SCSS processing. You can \u003ca href=\"https://github.com/gohugoio/hugo/releases\"\u003edownload it here\u003c/a\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"2-selecting-a-theme\"\u003e2. Selecting a Theme\u003c/h4\u003e\n\u003cp\u003eChoosing the right theme for your blog is crucial. Hugo offers a wide selection of themes, both in its official theme gallery and on platforms like GitHub. Spend some time exploring themes that resonate with your style and content. For my purposes I chose \u003ca href=\"https://github.com/CaiJimmy/hugo-theme-stack\"\u003eJimmy\u0026rsquo;s Stack theme\u003c/a\u003e as I liked it\u0026rsquo;s design and it is exactly what I needed from my blog.\u003c/p\u003e\n\u003ch4 id=\"3-cloning-and-customizing-the-theme\"\u003e3. Cloning and Customizing the Theme\u003c/h4\u003e\n\u003cp\u003eOnce I had found the perfect theme, I had clone its repository. Stack\u0026rsquo;s README was more than sufficient to get me started with this. After cloning and setting up, I started customizing the theme by modifying templates, colors, fonts, and adding my own branding elements along with my own social links and avatar.\u003c/p\u003e\n\u003ch4 id=\"4-pushing-to-github\"\u003e4. Pushing to GitHub\u003c/h4\u003e\n\u003cp\u003eAfter making the changes I needed, I pushed the site to GitHub. I had expected it to work out of the box just as one might expect with Jekyll but in case of Hugo it didn\u0026rsquo;t go as expected. I had to make some changes which I have described in the next step.\u003c/p\u003e\n\u003ch4 id=\"deploying-on-github-pages\"\u003eDeploying on GitHub Pages\u003c/h4\u003e\n\u003cp\u003eAfter going through Hugo\u0026rsquo;s documentation for GitHub pages, I got to know that in order to deploy my site automatically with GitHub pages I need to setup actions and provide it with a custom YAML which was given in Hugo\u0026rsquo;s documentation \u003ca href=\"https://gohugo.io/hosting-and-deployment/hosting-on-github/\"\u003ehere\u003c/a\u003e. After going through the given steps and mentioning the branch which I would be using for deployment, I was able to setup an automatic routine for deployment. Now, each time I make a push to this repo, it will be automatically built and deployed. :grin:\u003c/p\u003e\n\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eWith these steps, I was well on my way to creating a personal blog that would be a canvas for my thoughts, experiences, and creative expressions. My journey to create a personal blog with Hugo and GitHub Pages was a fulfilling experience. It allowed me to exercise my creativity, establish a digital presence, and embrace the freedom of self-expression. The decision to choose Hugo, with its speed, flexibility, and vibrant community, felt like the right one for my journey of self-expression and creativity. As I embark on this blogging adventure, I look forward to sharing my thoughts, insights, and experiences with the world while maintaining complete control over my digital space. This \u0026ldquo;Hello World\u0026rdquo; blog is just the beginning of an exciting journey in the blogosphere and a starting point for maintaining my jorunal about LLM and Generative AI research. Cheers :v:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePhoto by \u003ca href=\"https://unsplash.com/@pawel_czerwinski\"\u003ePawel Czerwinski\u003c/a\u003e on \u003ca href=\"https://unsplash.com/\"\u003eUnsplash\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n","description":"Setup Hugo + GitHub Pages based personal blog","image":"/blogs/hello-world/cover.jpg","permalink":"http://localhost:1313/blogs/hello-world/","title":"Embracing Freedom: My Journey to Creating a Personal Blog with Hugo and GitHub Pages"}]